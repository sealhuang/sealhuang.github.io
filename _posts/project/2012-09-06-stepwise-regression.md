---
layout: post
title: Stepwise regression 学习笔记
category: project
description: SPSS 中与 Regression 同时出现的方法。
---

之前在 SPSS 中的回归分析算法中发现，在它里面实现的算法有 Enter 和 Stepwise 两种。Enter 很容易理解，就是将所有选定的自变量一起放入模型中，直接去计算包含所有自变量的整个模型能够解释多少因变量中的变异，以及各个自变量单独的贡献有多少。但对 Stepwise regression 的理解总是很模糊，今天仔细查了一下，做下笔记。

与平时所说的 regression analysis 不太相同，stepwise regression 可以算是一种 feature extraction 的方法。

举个例子，假如我们的数据中有一个因变量，但却有十几或几十个自变量。为了便于对变量数过多的数据进行处理，避免 “curse of dimensionality” 中可能出现的种种问题，我们总是会对数据进行降维，根据在特定领域中的知识或是理论假设，选择其中一些可能更有意义的变量进行后续分析。但不是任何情况下我们都掌握这些先验信息，所以基于数据本身的特征提取方法应运而生。

在 stepwise regression 中，提取哪些变量主要基于的假设是：在线性条件下，哪些变量组合能够解释更多的因变量变异，则将其保留。

具体操作方法有三种：

- `Forward selection`: 首先模型中只有一个单独解释因变量变异最大的自变量，之后尝试将加入另一自变量，看加入后整个模型所能解释的因变量变异是否显著增加（这里需要进行检疫，可以用 F-test， t-test 等等）；这一过程反复迭代，直到没有自变量再符合加入模型的条件。
- `Backward elimination`: 与 Forward selection 相反，此时，所有变量均放入模型，之后尝试将其中一个自变量从模型中剔除，看整个模型解释因变量的变异是否有显著变化，之后将使解释量减少最少的变量剔除；此过程不断迭代，直到没有自变量符合剔除的条件。
- `Bidirectional elimination`: 这种方法相当于将前两种结合起来。可以想象，如果采用第一种方法，每加入一个自变量，可能会使已存在于模型中的变量单独对因变量的解释度减小，当其的作用很小（不显著）时，则可将其从模型中剔除。而第三种方法就做了这么一件事，不是一味的增加变量，而是增加一个后，对整个模型中的所有变量进行检验，剔除作用不显著的变量。最终尽可能得到一个最优的变量组合。

可以想象，这样得到的变量组合，基于当前数据，应该是可以最大程度的解释因变量的变异，但其反面的作用就是会使模型有偏，即所谓的 overfitting 问题；另外，鉴于算法是基于变量解释度来进行特征提取的，当两个变量对因变量的影响相近时，则不免受到较大的噪声影响，使特征提取结果不稳定。

`注`：上面的描述可能有些偏差或错误，具体内容大家可以移步[wiki][wiki_link].

[wiki_link]: http://en.wikipedia.org/wiki/Stepwise_regression "Stepwise regression"
